{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bitsandbytes\n!pip install peft\n!pip install accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = \"new_file.txt\"\ntext_content = \"\"\"\nThis is the first document.\nHere is another example of text data.\nUnlabeled data is used for unsupervised learning.\nFine-tuning a model on new data can improve its performance.\nMachine learning models require a lot of data to train effectively.\nNatural language processing is a field of artificial intelligence.\nDeep learning models can learn complex patterns from data.\nTransformer models have revolutionized NLP tasks.\nLanguage models can generate human-like text.\nPre-trained models can be fine-tuned on specific tasks.\nFine-tuning helps adapt models to new domains and tasks.\nText data can be sourced from various domains and contexts.\nLarge-scale datasets are crucial for training robust models.\nUnsupervised learning does not require labeled data.\nSelf-supervised learning is a powerful approach in NLP.\nThe quality of text data affects the performance of language models.\nTokenization is an important step in preprocessing text data.\nSequence models can handle variable-length text inputs.\nAttention mechanisms enable models to focus on relevant parts of the input.\nLanguage models can be evaluated on various benchmarks.\nText generation is a common application of language models.\nSentiment analysis is a popular task in NLP.\nContextual embeddings capture the meaning of words in context.\nText classification is used to categorize text into predefined classes.\nEntity recognition identifies named entities in text.\nLanguage models can be used for text summarization.\nData augmentation techniques can enhance the training process.\nFine-tuning requires careful selection of hyperparameters.\nPreprocessing text data involves cleaning and normalizing it.\nLanguage models can be adapted to different languages.\n\"\"\"\n# Open the file in write mode\nwith open(file_path, 'w') as file:\n    # Write content to the file\n    file.write(text_content)\n     \nprint(f\"File '{file_path}' created successfully.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nmodel_name='TinyLlama/TinyLlama-1.1B-Chat-v1.0'\ntokenizer = AutoTokenizer.from_pretrained(model_name)#\n#model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\")\n \n#Quantisation\nfrom transformers import BitsAndBytesConfig, Trainer, TrainingArguments\n# Configure the model for 8-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True,  # For 8-bit quantization\n    load_in_4bit=False  # Set to True if you want 4-bit quantization\n)\n# Configure the model for 8-bit quantization\n#bnb_config = BitsAndBytesConfig.from_pretrained(model_name, load_in_8bit=True)\n \n# Load the quantized model\nmodel = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n##Adding trainable quantised peft adapter which will help in transfer learning through layer freezing\nfrom peft import LoraConfig\n \npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel.add_adapter(peft_config)\n# # Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model.to(device)\n \nimport torch\nfrom transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n \n# Function to load dataset\ndef load_dataset(file_path, tokenizer, block_size=128):\n    dataset = TextDataset(\n        tokenizer=tokenizer,\n        file_path=file_path,\n        block_size=block_size\n    )\n    return dataset\n \n# Function to create data collator\ndef create_data_collator(tokenizer):\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,  # Masked Language Modeling (MLM) is false for autoregressive models like GPT-2\n    )\n    return data_collator\n \n# Load dataset\ntrain_dataset = load_dataset(\"/kaggle/working/new_file.txt\", tokenizer)\n \n# Create data collator\ndata_collator = create_data_collator(tokenizer)\n \n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    overwrite_output_dir=True,\n    num_train_epochs=20,\n    per_device_train_batch_size=8,\n    save_steps=10_000,\n    save_total_limit=2,\n    prediction_loss_only=True,\n)\n \n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n)\n ","metadata":{"execution":{"iopub.status.busy":"2024-06-05T12:47:33.611381Z","iopub.execute_input":"2024-06-05T12:47:33.611685Z","iopub.status.idle":"2024-06-05T12:47:52.872508Z","shell.execute_reply.started":"2024-06-05T12:47:33.611656Z","shell.execute_reply":"2024-06-05T12:47:52.871368Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-05 12:47:37.575511: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-05 12:47:37.575574: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-05 12:47:37.577007: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12830f624a794217940c0652b817b9be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea7336ca61b24997940a17ac2a3ca7a9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-05T12:48:15.235882Z","iopub.execute_input":"2024-06-05T12:48:15.236671Z","iopub.status.idle":"2024-06-05T12:49:23.735435Z","shell.execute_reply.started":"2024-06-05T12:48:15.236637Z","shell.execute_reply":"2024-06-05T12:49:23.734113Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240605_124857-0wdl26ks</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/my_team123/huggingface/runs/0wdl26ks' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/my_team123/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/my_team123/huggingface' target=\"_blank\">https://wandb.ai/my_team123/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/my_team123/huggingface/runs/0wdl26ks' target=\"_blank\">https://wandb.ai/my_team123/huggingface/runs/0wdl26ks</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [20/20 00:06, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=20, training_loss=1.7415674209594727, metrics={'train_runtime': 68.0911, 'train_samples_per_second': 0.587, 'train_steps_per_second': 0.294, 'total_flos': 32057044500480.0, 'train_loss': 1.7415674209594727, 'epoch': 20.0})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"./fine-tuned-llama\")\ntokenizer.save_pretrained(\"./fine-tuned-llama\")","metadata":{"execution":{"iopub.status.busy":"2024-06-05T12:51:17.430089Z","iopub.execute_input":"2024-06-05T12:51:17.430833Z","iopub.status.idle":"2024-06-05T12:51:17.762014Z","shell.execute_reply.started":"2024-06-05T12:51:17.430801Z","shell.execute_reply":"2024-06-05T12:51:17.760848Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/integrations/peft.py:399: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"('./fine-tuned-llama/tokenizer_config.json',\n './fine-tuned-llama/special_tokens_map.json',\n './fine-tuned-llama/tokenizer.model',\n './fine-tuned-llama/added_tokens.json',\n './fine-tuned-llama/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Load the model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(\"./fine-tuned-llama\")\ntokenizer = AutoTokenizer.from_pretrained(\"./fine-tuned-llama\")\n\n# Check if the tokenizer is using padding side left or right\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Generate a response\nprompt = \"why text classfication is used\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# Generate the model's response\noutput = model.generate(**inputs, max_length=100)\n\n# Decode the output\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(response,\".........\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T12:54:49.275340Z","iopub.execute_input":"2024-06-05T12:54:49.276039Z","iopub.status.idle":"2024-06-05T12:54:53.535674Z","shell.execute_reply.started":"2024-06-05T12:54:49.276006Z","shell.execute_reply":"2024-06-05T12:54:53.534585Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"why text classfication is used in the context of image recognition. .........\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"what is mean by machine learning\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# Generate the model's response\noutput = model.generate(**inputs, max_length=100)\n\n# Decode the output\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(response,\".........\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-05T12:56:33.630045Z","iopub.execute_input":"2024-06-05T12:56:33.630890Z","iopub.status.idle":"2024-06-05T12:56:36.927114Z","shell.execute_reply.started":"2024-06-05T12:56:33.630856Z","shell.execute_reply":"2024-06-05T12:56:36.926160Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"what is mean by machine learning and how it can be used in the field of finance. .........\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}